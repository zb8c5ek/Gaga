#
# Copyright (C) 2024, Gaga
# Gaga research group, https://github.com/weijielyu/Gaga
# All rights reserved.
#
# ------------------------------------------------------------------------
# Modified from codes in Gaussian-Grouping
# Gaussian-Grouping research group, https://github.com/lkeab/gaussian-grouping

import json
import os
import sys
import uuid
from argparse import ArgumentParser, Namespace
from random import randint

import torch
import wandb
from tqdm import tqdm

from arguments import ModelParams, PipelineParams, OptimizationParams
from gaussian_renderer import render, network_gui
from scene import Scene, GaussianModel
from utils.general_utils import safe_state
from utils.loss_utils import l1_loss, loss_cls_3d


def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from,
             use_wandb):
    first_iter = 0
    prepare_output_and_logger(dataset)
    gaussians = GaussianModel(dataset.sh_degree)
    scene = Scene(dataset, gaussians, load_iteration=-1, shuffle=True)
    gaussians.training_seg_only_setup(opt)
    # Get the number of classes
    matched_mask_path = os.path.join(dataset.source_path, dataset.object_path)
    info = json.load(open(os.path.join(matched_mask_path, "info.json")))
    print("Info of the mask association process: ", info)
    num_classes = info["num_mask"] + 1
    # change to half precision, as in 8G machine, GMEM explodes
    # classifier = torch.nn.Conv2d(gaussians.num_objects, num_classes, kernel_size=1).half()
    classifier = torch.nn.Conv2d(gaussians.num_objects, num_classes, kernel_size=1)
    cls_criterion = torch.nn.CrossEntropyLoss(reduction='none')
    cls_optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)
    classifier.cuda()

    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]
    background = torch.tensor(bg_color, dtype=torch.float32, device="cuda")

    iter_start = torch.cuda.Event(enable_timing=True)
    iter_end = torch.cuda.Event(enable_timing=True)

    viewpoint_stack = None
    ema_loss_for_log = 0.0
    progress_bar = tqdm(range(first_iter, opt.iterations), desc="Training progress")
    first_iter += 1
    for iteration in range(first_iter, opt.iterations + 1):
        # print(iteration)
        if network_gui.conn == None:
            network_gui.try_connect()
        while network_gui.conn != None:
            try:
                net_image_bytes = None
                custom_cam, do_training, pipe.convert_SHs_python, pipe.compute_cov3D_python, keep_alive, scaling_modifer = network_gui.receive()
                if custom_cam != None:
                    net_image = render(custom_cam, gaussians, pipe, background, scaling_modifer)["render"]
                    net_image_bytes = memoryview((torch.clamp(net_image, min=0, max=1.0) * 255).byte().permute(1, 2,
                                                                                                               0).contiguous().cpu().numpy())
                network_gui.send(net_image_bytes, dataset.source_path)
                if do_training and ((iteration < int(opt.iterations)) or not keep_alive):
                    break
            except Exception as e:
                network_gui.conn = None

        iter_start.record()

        gaussians.update_learning_rate(iteration)

        # Pick a random Camera
        if not viewpoint_stack:
            viewpoint_stack = scene.getTrainCameras().copy()
        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))

        # Render
        if (iteration - 1) == debug_from:
            pipe.debug = True
        render_pkg = render(viewpoint_cam, gaussians, pipe, background)
        image, viewspace_point_tensor, visibility_filter, radii, objects = render_pkg["render"], render_pkg[
            "viewspace_points"], render_pkg["visibility_filter"], render_pkg["radii"], render_pkg["render_seg"]

        # Object Loss
        gt_obj = viewpoint_cam.objects.cuda().long()

        # change to half precision, as in 8G machine, GMEM explodes
        # logits = classifier(objects.half())
        logits = classifier(objects)

        loss_obj = cls_criterion(logits.unsqueeze(0), gt_obj.unsqueeze(0)).squeeze().mean()

        # try:
        #     loss_obj = cls_criterion(logits.unsqueeze(0), gt_obj.unsqueeze(0)).squeeze().mean()
        # except:
        #     _, h, w = logits.shape
        #     gt_obj = torch.nn.functional.interpolate(gt_obj.unsqueeze(0).unsqueeze(0).float(), size=(h, w), mode="nearest").long().squeeze(0).squeeze(0)
        #     loss_obj = cls_criterion(logits.unsqueeze(0), gt_obj.unsqueeze(0)).squeeze().mean()

        loss_obj = loss_obj / torch.log(torch.tensor(num_classes))  # normalize to (0,1)

        loss_obj_3d = None
        if iteration % opt.reg3d_interval == 0:
            # regularize at certain intervals
            # Change to half precision, as in 8G machine, GMEM explodes
            # logits3d = classifier(gaussians._objects_dc.permute(2, 0, 1).half())
            logits3d = classifier(gaussians._objects_dc.permute(2, 0, 1))
            prob_obj3d = torch.softmax(logits3d, dim=0).squeeze().permute(1, 0)
            loss_obj_3d = loss_cls_3d(
                # gaussians._xyz.squeeze().detach(),
                gaussians._xyz.squeeze().detach().half(),
                prob_obj3d, opt.reg3d_k, opt.reg3d_lambda_val,
                opt.reg3d_max_points, opt.reg3d_sample_size
            )
            loss = loss_obj + loss_obj_3d
            print(loss.item())
            # torch.cuda.empty_cache()
        else:
            loss = loss_obj

        loss.backward()
        # torch.cuda.empty_cache()
        iter_end.record()

        with torch.no_grad():
            # Progress bar
            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log
            # try:
            #     ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log
            # except:
            #     ema_loss_for_log = 0.0
            if iteration % 10 == 0:
                progress_bar.set_postfix({"Loss": f"{ema_loss_for_log:.{7}f}"})
                progress_bar.update(10)
            if iteration == opt.iterations:
                progress_bar.close()

            # Log and save
            try:
                training_report(iteration, loss_obj, loss, l1_loss, iter_start.elapsed_time(iter_end),
                                testing_iterations, scene, render, (pipe, background), loss_obj_3d, use_wandb)
            except:
                pass
            if (iteration in saving_iterations):
                print("\n[ITER {}] Saving Gaussians".format(iteration))
                scene.save(iteration)
                torch.save(classifier.state_dict(),
                           os.path.join(scene.model_path, "point_cloud/iteration_{}".format(iteration),
                                        'classifier.pth'))

            # Optimizer step
            try:
                if iteration < opt.iterations:
                    gaussians.optimizer.step()
                    gaussians.optimizer.zero_grad(set_to_none=True)
                    cls_optimizer.step()
                    cls_optimizer.zero_grad()
            except:
                print("Error in optimizer step")
                pass

            if (iteration in checkpoint_iterations):
                print("\n[ITER {}] Saving Checkpoint".format(iteration))
                torch.save((gaussians.capture(), iteration), scene.model_path + "/chkpnt" + str(iteration) + ".pth")


def prepare_output_and_logger(args):
    if not args.model_path:
        if os.getenv('OAR_JOB_ID'):
            unique_str = os.getenv('OAR_JOB_ID')
        else:
            unique_str = str(uuid.uuid4())
        args.model_path = os.path.join("./output/", unique_str[0:10])

    # Set up output folder
    print("Output folder: {}".format(args.model_path))
    os.makedirs(args.model_path, exist_ok=True)
    with open(os.path.join(args.model_path, "cfg_args"), 'w') as cfg_log_f:
        cfg_log_f.write(str(Namespace(**vars(args))))


def training_report(iteration, loss_obj, loss, l1_loss, elapsed, testing_iterations, scene: Scene, renderFunc,
                    renderArgs, loss_obj_3d, use_wandb):
    if use_wandb:
        if loss_obj_3d:
            wandb.log({"train_loss_patches/total_loss": loss.item(), "train_loss_patches/loss_obj": loss_obj.item(),
                       "train_loss_patches/loss_obj_3d": loss_obj_3d.item(), "iter_time": elapsed, "iter": iteration})
        else:
            wandb.log({"train_loss_patches/total_loss": loss.item(), "train_loss_patches/loss_obj": loss_obj.item(),
                       "iter_time": elapsed, "iter": iteration})

    torch.cuda.empty_cache()


if __name__ == "__main__":
    # Set up command line argument parser
    parser = ArgumentParser(description="Training script parameters")
    lp = ModelParams(parser)
    op = OptimizationParams(parser)
    pp = PipelineParams(parser)
    parser.add_argument('--ip', type=str, default="127.0.0.1")
    parser.add_argument('--port', type=int, default=6009)
    parser.add_argument('--debug_from', type=int, default=-1)
    parser.add_argument('--detect_anomaly', action='store_true', default=False)
    parser.add_argument("--test_iterations", nargs="+", type=int, default=[10_000])
    parser.add_argument("--save_iterations", nargs="+", type=int, default=[10_000])
    parser.add_argument("--quiet", action="store_true")
    parser.add_argument("--checkpoint_iterations", nargs="+", type=int, default=[10_000])
    parser.add_argument("--start_checkpoint", type=str, default=None)
    # Add an argument for the configuration file
    parser.add_argument("--config_file", type=str, default="config/train.json", help="Path to the configuration file")
    parser.add_argument("--use_wandb", action='store_true', default=False, help="Use wandb to record loss value")
    parser.add_argument("--my_debug_tag", action='store_true', default=False, help="Debug tag for my own purpose")

    args = parser.parse_args(sys.argv[1:])
    args.save_iterations.append(args.iterations)

    # temproray solution
    assert args.lift == False
    args.lift = True

    # Read and parse the configuration file
    try:
        with open(args.config_file, 'r') as file:
            config = json.load(file)
    except FileNotFoundError:
        print(f"Error: Configuration file '{args.config_file}' not found.")
        exit(1)
    except json.JSONDecodeError as e:
        print(f"Error: Failed to parse the JSON configuration file: {e}")
        exit(1)

    args.densify_until_iter = config.get("densify_until_iter", 15000)
    args.num_classes = config.get("num_classes", 200)  # 200
    args.reg3d_interval = config.get("reg3d_interval", 2)
    args.reg3d_k = config.get("reg3d_k", 5)
    args.reg3d_lambda_val = config.get("reg3d_lambda_val", 2)
    args.reg3d_max_points = config.get("reg3d_max_points", 300000)  # 30_0000
    args.reg3d_sample_size = config.get("reg3d_sample_size", 1000)

    print("Optimizing " + args.model_path)

    if args.use_wandb:
        wandb.init(project="Gaga")
        wandb.config.args = args
        run_name = "_".join(args.model_path.split("/")[1:])
        wandb.run.name = run_name
    # Initialize system state (RNG)
    safe_state(args.quiet)

    # Start GUI server, configure and run training
    # network_gui.init(args.ip, args.port)
    torch.autograd.set_detect_anomaly(args.detect_anomaly)
    training(
        lp.extract(args), op.extract(args),
        pp.extract(args), args.test_iterations, args.save_iterations,
        args.checkpoint_iterations, args.start_checkpoint,
        args.debug_from, args.use_wandb
    )

    # All done
    print("\nTraining complete.")
